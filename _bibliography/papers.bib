---
---
@inproceedings{meode,
    abbr={arXiv},
    %teaser={/publications/me_ode2.pdf},
    teaser={/publications/ordinal-quadruplet/ordin_quadr.gif},
    title = {Ordinal-Quadruplet: Retrieval of Missing Classes in Ordinal Time Series},
    author = {Jurijs Nazarovs, Cristian Lumezanu, Qianying Ren, Yuncong Chen, Takehiko Mizoguchi, Dongjin Song, Haifeng Chen},
    abstract = {In this paper, we propose an ordered time series classification framework that is robust against missing classes in the training data, i.e., during testing we can prescribe classes that are missing during training. This framework relies on two main components: (1) our newly proposed ordinal-quadruplet loss, which forces the model to learn latent representation while preserving the ordinal relation among labels, (2) testing procedure, which utilizes the property of latent representation (order preservation). We conduct experiments based on real world multivariate time series data and show the significant improvement in the prediction of missing labels even with 40\% of the classes are missing from training. Compared with the well-known triplet loss optimization augmented with interpolation for missing information, in some cases, we nearly double the accuracy.},
  year = {2022},
  //confname={Thirty-seventh Conference on Uncertainty in Artificial Intelligence},
  //acceptrate={26},
  //month = {May},
  arxiv={2201.09907},
  //pdf={/publications/meode/main.pdf},
  //supp={adios-icml16-supp.pdf},
  //poster={/publications/meode/poster.pdf},
  //video={https://youtu.be/Nmum5urconQ}, 
  //code={https://github.com/JurijsNazarovs/panel_me_ode},

}

@inproceedings{meode,
    abbr={UAI},
    %teaser={/publications/me_ode2.pdf},
    teaser={/publications/meode/tadpole.gif},
    title = {Mixed Effect Neural ODE: A variational approximation for analyzing the dynamics of panel data},
    author = {Nazarovs, Jurijs and Chakraborty, Rudrasis and  Tasneeyapant, Songwong  and Ravi, Sathya and Singh, Vikas},
    abstract = {   Panel data involving longitudinal measurements of the same set of participants or entities taken over multiple time points is common in studies to understand early childhood development and disease modeling. Deep hybrid models that marry the predictive power of neural networks with physical 
simulators such as differential equations, are starting to drive 
advances in such applications. The task of modeling 
not just the observations/data but the hidden dynamics that are captured by the measurements poses interesting statistical/computational questions. 
We propose a probabilistic model called ME-NODE to incorporate (fixed + random) mixed effects for analyzing such panel data. 
We show that our model can be derived using smooth approximations of SDEs provided by the Wong-Zakai theorem. We then derive Evidence Based Lower Bounds for ME-NODE, and develop (efficient) training algorithms using MC based sampling methods and numerical ODE solvers. 
We demonstrate ME-NODE's utility on tasks
spanning the spectrum from simulations and toy datasets to real longitudinal 3D imaging data from an Alzheimer's disease (AD) study, and study the performance for accuracy of reconstruction 
for interpolation, uncertainty estimates and personalized prediction.},
  year = {2021},
  //confname={Thirty-seventh Conference on Uncertainty in Artificial Intelligence},
  acceptrate={26},
  month = {May},
  //arxiv={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  pdf={/publications/meode/main.pdf},
  //supp={adios-icml16-supp.pdf},
  poster={/publications/meode/poster.pdf},
  video={https://youtu.be/Nmum5urconQ},
  code={https://github.com/JurijsNazarovs/panel_me_ode},

}

@inproceedings{mcrepar,
    abbr={UAI},
    teaser={/publications/mcrepar/graph.gif},
    title = {Graph reparameterizations for enabling 1000+ monte carlo iterations in bayesian deep neural networks},
    author = {Nazarovs, Jurijs and Mehta, Ronak R. and Lokhande, Vishnu Suresh and Singh, Vikas},
    abstract = {    Uncertainty estimation in deep models 
    is essential in many real-world applications and has benefited 
    from developments over the last several years.  
    Recent evidence  suggests that existing solutions dependent on simple Gaussian formulations may not be sufficient.
    However, moving to other distributions necessitates Monte Carlo (MC) sampling to estimate quantities such as the KL divergence: it could be expensive and scales poorly as the dimensions of both the input data and the model grow. 
    This is directly related to the structure of the computation graph, which can grow linearly as a function of the number of MC samples needed.
    Here, we construct a framework to describe these computation graphs, and identify probability families where the graph size can be independent or only weakly dependent on the number of MC samples.
    These families correspond directly to large classes of distributions.
    Empirically, we can run a much larger number of iterations for MC approximations for larger architectures used in 
    computer vision
    with gains in performance measured in confident accuracy, stability of training, memory and training time.},


  year={2021},
  //confname={Thirty-seventh Conference on Uncertainty in Artificial Intelligence},
  
  acceptrate={26},
  month={May},
  //arxiv={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  pdf={/publications/mcrepar/main.pdf},
  //supp={adios-icml16-supp.pdf},
  poster={/publications/mcrepar/poster.pdf},
  video={https://youtu.be/Vg5La64V9Bs},
  code={https://github.com/JurijsNazarovs/bayesian_nn},
}


