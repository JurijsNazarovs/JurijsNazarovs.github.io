---
---

@inproceedings{meode,
    abbr={UAI},
    title = {A variational approximation for analyzing the dynamics of panel data},
    author = {Nazarovs, Jurijs and Chakraborty, Rudrasis and  Tasneeyapant, Songwong  and Ravi, Sathya and Singh, Vikas},
    abstract = {   Panel data involving longitudinal measurements of the same set of participants or entities taken over multiple time points is common in studies to understand early childhood development and disease modeling. Deep hybrid models that marry the predictive power of neural networks with physical 
simulators such as differential equations, are starting to drive 
advances in such applications. The task of modeling 
not just the observations/data but the hidden dynamics that are captured by the measurements poses interesting statistical/computational questions. 
We propose a probabilistic model called ME-NODE to incorporate (fixed + random) mixed effects for analyzing such panel data. 
We show that our model can be derived using smooth approximations of SDEs provided by the Wong-Zakai theorem. We then derive Evidence Based Lower Bounds for ME-NODE, and develop (efficient) training algorithms using MC based sampling methods and numerical ODE solvers. 
We demonstrate ME-NODE's utility on tasks
spanning the spectrum from simulations and toy datasets to real longitudinal 3D imaging data from an Alzheimer's disease (AD) study, and study the performance for accuracy of reconstruction 
for interpolation, uncertainty estimates and personalized prediction.},
  year = {2021},
  month = {May},
  //arxiv={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  pdf={nazarovs_58.pdf},
  //supp={adios-icml16-supp.pdf},
  poster={nazarovs_58_poster.pdf},
  video={https://youtu.be/Nmum5urconQ},
  //code={https://github.com/alshedivat/adios},

}

@inproceedings{mcrepar,
    abbr={UAI},
    title = {Graph reparameterizations for enabling 1000+ monte carlo iterations in bayesian deep neural networks},
    author = {Nazarovs, Jurijs and Mehta, Ronak R. and Lokhande, Vishnu Suresh and Singh, Vikas},
    abstract = {    Uncertainty estimation in deep models 
    is essential in many real-world applications and has benefited 
    from developments over the last several years.  
    Recent evidence  suggests that existing solutions dependent on simple Gaussian formulations may not be sufficient.
    However, moving to other distributions necessitates Monte Carlo (MC) sampling to estimate quantities such as the KL divergence: it could be expensive and scales poorly as the dimensions of both the input data and the model grow. 
    This is directly related to the structure of the computation graph, which can grow linearly as a function of the number of MC samples needed.
    Here, we construct a framework to describe these computation graphs, and identify probability families where the graph size can be independent or only weakly dependent on the number of MC samples.
    These families correspond directly to large classes of distributions.
    Empirically, we can run a much larger number of iterations for MC approximations for larger architectures used in 
    computer vision
    with gains in performance measured in confident accuracy, stability of training, memory and training time.},


  year={2021},
  month={May},
  //arxiv={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  pdf={nazarovs_59.pdf},
  //supp={adios-icml16-supp.pdf},
  poster={nazarovs_59_poster.pdf},
  video={https://youtu.be/Vg5La64V9Bs},
  //code={https://github.com/alshedivat/adios},
}


